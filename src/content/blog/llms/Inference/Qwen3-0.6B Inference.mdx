---
title: æ‰‹å†™Qwen3-0.6Bæ¨ç†
description: ä¸€æ–‡äº†è§£LLMæ¨ç†åº•å±‚é€»è¾‘ï¼Œæ‰‹å†™æ¨ç†demo
image: ../../blog_post.jpg
publishDate: 2025-11-19
tags:
  - LLM
  - Inference
---

# æ‰‹å†™Qwen3-0.6Bæ¨ç†

## ç®€ä»‹

æœ¬æ–‡å°†ä½¿ç”¨Pytorchä»0æ‰‹å†™Qwen3-0.6Bæ¨ç†demoï¼ŒåŒ…å«Qwen3-0.6Bæ¨¡å‹ç»“æ„åˆ†æç†è§£
ã€å®ç°Qwen3-0.6Bæ•´ä½“ç»“æ„ã€å®ç°åˆ†ç»„æ³¨æ„åŠ›æœºåˆ¶ä¸æ—‹è½¬ä½ç½®ç¼–ç ç­‰é‡è¦å†…å®¹ï¼Œé™„æœ‰githubä»“åº“ï¼ŒåŒ…å«jupyter notebookç­‰æ–‡ä»¶ä¾›äº¤äº’å¼è¿è¡Œã€‚githubä»“åº“é“¾æ¥å¦‚ä¸‹ï¼šhttps://github.com/ComistryMo/Inference_demo

## ä¸»è¦å†…å®¹

### ä¸‹è½½æ¨¡å‹

```python
!hf download Qwen/Qwen3-0.6B --local-dir ./Qwen3-0.6B
```
å¦‚æœæ— æ³•ä¸‹è½½ï¼Œå¯ä»¥ä½¿ç”¨hf-mirrorä»£ç†è¿›è¡Œä¸‹è½½
### æ‰“å°æ¨¡å‹ç»“æ„

```python
from safetensors.torch import load_file
state_dict = load_file('./Qwen3-0.6B/model.safetensors')
for k,p in state_dict.items():
	print(f"key:{k} shape:{p.shape}")
```
ä½¿ç”¨æœ¬æ®µä»£ç å¯ä»¥æ‰“å°å‡ºæ¨¡å‹çš„ç»“æ„ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
```python
key:lm_head.weight shape:torch.Size([151936, 1024])
key:model.embed_tokens.weight shape:torch.Size([151936, 1024]) 
key:model.layers.0.input_layernorm.weight shape:torch.Size([1024]) 
key:model.layers.0.mlp.down_proj.weight shape:torch.Size([1024, 3072]) 
key:model.layers.0.mlp.gate_proj.weight shape:torch.Size([3072, 1024]) 
key:model.layers.0.mlp.up_proj.weight shape:torch.Size([3072, 1024]) 
key:model.layers.0.post_attention_layernorm.weight shape:torch.Size([1024]) 
key:model.layers.0.self_attn.k_norm.weight shape:torch.Size([128]) 
key:model.layers.0.self_attn.k_proj.weight shape:torch.Size([1024, 1024]) 
key:model.layers.0.self_attn.o_proj.weight shape:torch.Size([1024, 2048]) 
key:model.layers.0.self_attn.q_norm.weight shape:torch.Size([128]) 
key:model.layers.0.self_attn.q_proj.weight shape:torch.Size([2048, 1024]) 
key:model.layers.0.self_attn.v_proj.weight shape:torch.Size([1024, 1024]) 
key:model.layers.1.input_layernorm.weight shape:torch.Size([1024]) 
key:model.layers.1.mlp.down_proj.weight shape:torch.Size([1024, 3072]) 
key:model.layers.1.mlp.gate_proj.weight shape:torch.Size([3072, 1024]) 
key:model.layers.1.mlp.up_proj.weight shape:torch.Size([3072, 1024]) 
key:model.layers.1.post_attention_layernorm.weight shape:torch.Size([1024]) 
key:model.layers.1.self_attn.k_norm.weight shape:torch.Size([128]) 
key:model.layers.1.self_attn.k_proj.weight shape:torch.Size([1024, 1024]) 
key:model.layers.1.self_attn.o_proj.weight shape:torch.Size([1024, 2048]) 
key:model.layers.1.self_attn.q_norm.weight shape:torch.Size([128]) 
key:model.layers.1.self_attn.q_proj.weight shape:torch.Size([2048, 1024]) 
key:model.layers.1.self_attn.v_proj.weight shape:torch.Size([1024, 1024]) 
key:model.layers.10.input_layernorm.weight shape:torch.Size([1024])
...
key:model.layers.9.self_attn.q_norm.weight shape:torch.Size([128]) 
key:model.layers.9.self_attn.q_proj.weight shape:torch.Size([2048, 1024]) 
key:model.layers.9.self_attn.v_proj.weight shape:torch.Size([1024, 1024]) 
key:model.norm.weight shape:torch.Size([1024])
```
ç”±äºå†…å®¹å¤ªé•¿ï¼Œæ‰“å°å‡ºçš„ä¿¡æ¯ä¼šè¢«æŠ˜å ï¼Œä½†ä»å¯ä»¥å¯¹æ•´ä¸ªæ¨¡å‹çš„æ¶æ„ä¸€æ¢ç©¶ç«Ÿï¼š
- lm_headï¼šè¿™æ˜¯æ¨¡å‹æœ€å¤–å±‚çš„å¤´éƒ¨ï¼Œè´Ÿè´£è¾“å‡ºï¼Œä¸ºçº¿æ€§å˜æ¢ï¼Œä¸»è¦å°†æ¨¡å‹è¾“å‡ºæ˜ å°„ä¸ºè¯è¡¨æ¦‚ç‡ï¼Œshapeä¸º[vocab_size, hidden_size]
- modelï¼šé™¤äº†lm_headï¼Œå…¶ä½™å±‚éƒ½æœ‰å‰ç¼€modelï¼Œæˆ‘ä»¬ä¾æ¬¡æ‹†è§£ï¼š
	- embed_tokensï¼šå¤„ç†è¯åµŒå…¥ï¼Œå°†tokenè½¬åŒ–ä¸ºå¼ é‡ï¼Œshapeä¸º[vocab_size, hidden_size]
	- normï¼šå½’ä¸€åŒ–ï¼Œshapeä¸º[hidden_size]
	- layersï¼šä¹Ÿå°±æ˜¯Decoderå—ï¼Œä»”ç»†æ‹†è§£æ¯ä¸ªDecoderå†…éƒ¨çš„ç»“æ„ï¼š
		- input_layernormï¼šå¯¹è¾“å…¥å‚æ•°ä½œå½’ä¸€åŒ–ï¼Œshapeä¸º[hidden_size]
		- mlpï¼šæ˜¯FFNå±‚ï¼ŒåŒ…å«ä¸‰ä¸ªçº¿æ€§å˜æ¢éƒ¨åˆ†
		- self_attnï¼šè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ŒåŒ…å«QKVç›¸å…³å‚æ•°
			- projï¼šä¹Ÿå°±æ˜¯å¸¸è¯´çš„QKVçŸ©é˜µï¼Œshapeä¸º[hidden_size, hidden_size]ï¼Œç”±äºQwen3ä½¿ç”¨åˆ†ç»„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆGQAï¼‰ï¼ŒQæœ‰16ä¸ªå¤´ï¼ŒKVä¸º8å¤´ï¼Œæ¯ä¸¤ä¸ªQå…±äº«ä¸€ç»„KVï¼Œå› æ­¤åœ¨è®¡ç®—æ—¶Qçš„ç»´åº¦éœ€è¦æ”¹å˜ä¸º[num_attention_heads/num_key_value_heads\*hidden_size, hidden_size]ã€‚o_projä¸ºè¾“å‡ºæŠ•å½±ï¼Œå°†softmaxçš„ç»“æœæŠ•å½±ä¸ºåŸç»´åº¦
			- normï¼šä»…å¯¹QKå½’ä¸€åŒ–ï¼Œåœ¨æ¯ä¸ªå¤´å†…éƒ¨å‘ç”Ÿï¼Œå› æ­¤shapeä¸º[head_dim]
		- post_attention_layernormï¼šæ³¨æ„åŠ›ä¹‹åï¼ŒFFNä¹‹å‰çš„å½’ä¸€åŒ–ï¼Œshapeä¸º[hidden_size]
### æ‰“å°æ¨¡å‹ç»“æ„ï¼ˆè¯¦ç»†ï¼‰
å¯ä»¥ä½¿ç”¨ä¸€ä»½æ›´è¯¦ç»†çš„ä»£ç æ¥è¿›è¡Œè¾“å‡ºï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹åç§°ï¼Œå¤§å°ç­‰é¢å¤–ä¿¡æ¯ï¼Œè¿™é‡Œä¸å†å±•ç¤ºï¼Œview_model.pyè¯¦è§githubä»“åº“ï¼š
```python
from view_model import view_model_info
view_model_info("./Qwen3-0.6B/")
```

### å®šä¹‰æ¨¡å‹ç»“æ„
ç›®å‰æˆ‘ä»¬å·²ç»äº†è§£äº†Qwen3-0.6Bæ¨¡å‹çš„ç»“æ„ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å®ç°æ•´ä¸ªæ¶æ„ï¼Œæ‰èƒ½å°†æ¨¡å‹æƒé‡æ­£ç¡®åœ°åŠ è½½ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬éµå¾ªç”±æ•´ä½“åˆ°å±€éƒ¨çš„åŸåˆ™è¿›è¡Œå®ç°ï¼š
#### Qwen3Model
æ ¹æ®æˆ‘ä»¬ä¸Šè¿°æ‰“å°å‡ºæ¥çš„æ¨¡å‹ç»“æ„ï¼Œå¯ä»¥çœ‹åˆ°æ•´ä½“ä¸Šæˆ‘ä»¬æœ‰modelã€lm_headä¸¤éƒ¨åˆ†ï¼Œmodelä¸­åˆåˆ†ä¸ºnormã€embed_tokensã€layerså‡ éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç¼–ç å¦‚ä¸‹ï¼š
```python
from transformers import AutoConfig
import torch
import torch.nn as nn
import torch.nn.functional as F

class Qwen3Model(nn.Module):
	def __init__(self, config):
		print("begin init model")		
		super().__init__()
		self.config = config
		self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
		self.layers = nn.ModuleList([Qwen3DecoderLayer(config) for _ in range(config.num_hidden_layers)])
		self.norm = Qwen3RMSNorm(config.hidden_size)
		self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
		self.lm_head.weight = self.embed_tokens.weight
```
- æ‰€æœ‰ç±»å‡ç»§æ‰¿nn.Module
- embed_tokensä½¿ç”¨nnæä¾›çš„Embeddingå±‚å®ç°
- layersä¹Ÿå°±æ˜¯è§£ç å™¨å±‚ï¼Œæˆ‘ä»¬æ‰‹åŠ¨åœ¨åç»­éƒ¨åˆ†å®ç°
- normæˆ‘ä»¬æ‰‹åŠ¨åœ¨åç»­éƒ¨åˆ†å®ç°
- lm_headä¸ºçº¿æ€§å±‚ï¼Œä¸embed_tokenså…±äº«æƒé‡
å®ç°Qwen3Modelçš„forwardå‡½æ•°ï¼š
```python
def forward(self, input_ids):
	bsz, q_len = input_ids.shape
	pos_ids = torch.arange(q_len, dtype=torch.long, device=input_ids.device).unsqueeze(0)
	casual_mask = torch.triu(
		torch.full((q_len, q_len), float('-inf'), dtype=torch.float32, device=input_ids.device),
		diagonal=1
	).unsqueeze(0).unsqueeze(0).expand(bsz, 1, q_len, q_len)
	
	hidden_states = self.embed_tokens(input_ids)
	for layer in self.layers:
		hidden_states = layer(hidden_states, pos_ids=pos_ids, attn_mask=casual_mask)
	hidden_states = self.norm(hidden_states)
	logits = self.lm_head(hidden_states)
	return logits
```
- åœ¨forwardå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å®é™…ä¸Šå®Œæˆçš„æ˜¯æ¨ç†çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬æ¥å—è¾“å…¥ä¸ºinput_idsï¼Œè¿™æ˜¯åˆ†è¯çš„ç»“æœï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡embed_tokensè½¬æ¢ä¸ºéšè—å±‚å‘é‡ï¼Œç„¶åä¾æ¬¡ç»è¿‡è§£ç å™¨å±‚ï¼Œé€šè¿‡å½’ä¸€åŒ–åç»è¿‡lm_headè¾“å‡ºå¾—åˆ°logitsï¼Œç•™ä½œè§£ç ä½¿ç”¨ã€‚è¯¦ç»†è¿‡ç¨‹å‚è€ƒæœ¬å›¾ï¼š
![[Pasted image 20251120210942.png]]
- pos_idsæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸€æ¬¡unsqueezeæ¥æ‰©å……åˆ°(1, q_len)çš„ç»´åº¦ï¼Œæ–¹ä¾¿å¹¿æ’­
- å› ä¸ºQwen3ä½¿ç”¨çš„æ˜¯GQAï¼Œæˆ‘ä»¬çš„maskéœ€è¦åšä¸¤æ¬¡ç»´åº¦çš„æ‰©å¼ ä»¥é€‚é…å¤´æ•°ï¼Œç¬¬0ç»´å¯¹åº”batchsizeï¼Œç¬¬1ç»´å¯¹åº”head
#### Qwen3DecoderLayer
åœ¨å®ç°äº†æ•´ä½“æ¨¡å‹æ¶æ„åï¼Œæˆ‘ä»¬ç”±å¤–å‘å†…ï¼Œå‘ç°å…¶ä½™å‡ä¸ºè§£ç å™¨å±‚ï¼Œå¯¹è§£ç å™¨è¿›è¡Œåˆ†æï¼Œå…¶ä¸­åŒ…å«input_layernormã€mlpã€self_attnã€post_attention_layernormï¼Œæˆ‘ä»¬ç¼–ç å¦‚ä¸‹ï¼š
```python
class Qwen3DecoderLayer(nn.Module):
	def __init__(self, config):
		super().__init__()
		self.self_attn = Attention(config)
		self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size)
		self.mlp = MLP(config)
		self.input_layernorm = Qwen3RMSNorm(config.hidden_size)
	
	def forward(self, hidden_states, pos_ids=None, attn_mask=None):
		residual = hidden_states
		hidden_states = self.input_layernorm(hidden_states)
		hidden_states = self.self_attn(hidden_states, pos_ids, attn_mask)
		hidden_states = residual + hidden_states
		residual = hidden_states
		hidden_states = self.post_attention_layernorm(hidden_states)
		hidden_states = self.mlp(hidden_states)
		hidden_states = residual + hidden_states
		return hidden_states
```
- è§£ç å™¨çš„æµç¨‹å›¾å¦‚ä¸‹ï¼Œå‚è€ƒå›¾ç‰‡å¯ä»¥å¾ˆå®¹æ˜“çš„ç†è§£ä»£ç ï¼š
![[Pasted image 20251120210339.png]]
#### Qwen3RMSNorm
RMSNormåº”ç”¨åœ¨å„ä¸ªåœ°æ–¹ï¼Œæ”¶åˆ°çš„å‚æ•°å¤§å°æœ‰æ—¶å€™ä¸ºconfig.head_dimï¼Œæœ‰æ—¶å€™ä¸ºconfig.hidden_sizeï¼Œå› æ­¤ä¸ç›´æ¥æ¥æ”¶configå‚æ•°ï¼Œè€Œæ˜¯å°†ç»è¿‡çš„å‚æ•°å¤§å°è¿›è¡Œä¼ å…¥ï¼Œç¼–ç å¦‚ä¸‹ï¼š
```python
class Qwen3RMSNorm(nn.Module):
	def __init__(self, hidden_size, eps=1e-6):
		super().__init__()
		self.weight = nn.Parameter(torch.ones(hidden_size))
		self.eps = eps

	def forward(self, hidden_states):
		input_dtype = hidden_states.dtype
		hidden_states = hidden_states.to(torch.float32)
		var = hidden_states.pow(2).mean(-1, keepdim=True)
		hidden_states = hidden_states * torch.rsqrt(var + self.eps)
		rms_res = self.weight * hidden_states.to(input_dtype)
		return rms_res
```
- RMSNormå…¬å¼å¦‚ä¸‹ï¼š
![[Pasted image 20251120211211.png]]
#### Attention
åœ¨DecoderLayerä¸­ï¼Œæœ€é‡è¦çš„ä¸€éƒ¨åˆ†å°±æ˜¯Attentionï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å®ç°GQAï¼Œç¼–ç å¦‚ä¸‹ï¼š
```python
class Attention(nn.Module):
	def __init__(self, config):
		super().__init__()
		self.hidden_size = config.hidden_size
		self.num_heads = config.num_attention_heads
		self.num_kv_heads = config.num_key_value_heads
		self.head_dim = config.head_dim
		self.num_kv_groups = self.num_heads // self.num_kv_heads 
		self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)
		self.k_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)
		self.v_proj = nn.Linear(self.hidden_size, self.num_kv_heads * self.head_dim, bias=False)
		self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
		self.q_norm = Qwen3RMSNorm(config.head_dim, eps=config.rms_norm_eps)
		self.k_norm = Qwen3RMSNorm(config.head_dim, eps=config.rms_norm_eps)
		self.rope_theta = config.rope_theta
```
- åœ¨GQAä¸­ï¼Œä¸€ç»„KVå¯¹åº”å¤šä¸ªQï¼ŒKVçš„groupæ•°ä¸ºnum_heads // num_kv_heads
- å¯¹äºq_projï¼Œæˆ‘ä»¬åº”è¯¥å°†è¾“å…¥æ˜ å°„åˆ°self.num_heads * self.head_dimçš„å¤§å°ï¼Œå› ä¸ºæœ‰self.num_headsä¸ªQï¼Œæ¯ä¸ªQéƒ½å¯¹åº”ä¸€ä¸ªself.head_dim
- å¯¹äºk_projå’Œv_projï¼Œæˆ‘ä»¬åº”è¯¥å°†è¾“å…¥æ˜ å°„åˆ°self.num_kv_heads * self.head_dimçš„å¤§å°ï¼Œå› ä¸ºæœ‰self.num_kv_headsç»„KVï¼Œæ¯ç»„KVéƒ½å¯¹åº”ä¸€ä¸ªself.head_dim
- QKçš„å½’ä¸€åŒ–å‘ç”Ÿåœ¨æ¯ä¸ªheadå†…éƒ¨ï¼Œå› æ­¤å¤§å°ä¸ºself.head_dimå³å¯
GQAçš„forwardå‡½æ•°ï¼š
```python
def forward(self, hidden_states, pos_ids=None, attn_mask=None):
	bsz, q_len, _ = hidden_states.size()
	q = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
	k = self.k_proj(hidden_states).view(bsz, q_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
	v = self.v_proj(hidden_states).view(bsz, q_len, self.num_kv_heads, self.head_dim).transpose(1, 2)
	
	q = self.q_norm(q)
	k = self.k_norm(k)
	q, k = apply_rope(q, k, pos_ids, self.head_dim, self.rope_theta)
	
	if self.num_kv_groups > 1:
		k = k.unsqueeze(2).expand(-1, -1, self.num_kv_groups, -1, -1).flatten(1, 2)
		v = v.unsqueeze(2).expand(-1, -1, self.num_kv_groups, -1, -1).flatten(1, 2)
		
	attn_score = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
	
	if attn_mask is not None:
		attn_score = attn_score + attn_mask
		
	attn_score = torch.softmax(attn_score, dim=-1, dtype=torch.float32).to(hidden_states.dtype)
	attn_output = torch.matmul(attn_score, v)
	attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1)
	attn_output = self.o_proj(attn_output)
	return attn_output
```
- GQAçš„æµç¨‹å¦‚ä¸‹ï¼š
![[Pasted image 20251120211603.png]]
- æˆ‘ä»¬çš„è¾“å…¥sizeæ˜¯(bsz, q_len, hidden_size)ï¼Œç»è¿‡q_projåï¼Œå˜ä¸º(bsz, q_len, self.num_heads * self.head_dim)ï¼Œä½¿ç”¨viewåå˜ä¸º(bsz, q_len, self.num_heads, self.head_dim)ï¼Œtransposeåä¸º(bsz, self.num_heads, q_len, self.head_dim)ï¼Œè¿™æ˜¯ä¸ºäº†æ–¹ä¾¿åœ¨å¤´çš„å†…éƒ¨è¿›è¡Œå¹¿æ’­ã€‚å¯¹KVçš„æ“ä½œåŒç†
- å¦‚æœgroupså¤§äº1ï¼Œä¹Ÿå°±æ˜¯åœ¨GQAæˆ–è€…MQAåœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬éœ€è¦å¯¹é½ç»´åº¦ï¼Œæ‰èƒ½è¿›è¡Œç‚¹ç§¯è¿ç®—ï¼Œå…·ä½“æˆ‘ä»¬å°±æ˜¯å°†KVå¤´å¤åˆ¶/å¹¿æ’­ï¼Œä½¿å…¶æ•°é‡å¯¹é½Qå¤´ã€‚
- ç®€å•ç†è§£å°±æ˜¯KVçš„sizeæœ¬æ¥ä¸º(bsz, self.num_kv_heads, q_len, self.head_dim)ï¼Œç›¸æ¯”äºQçš„sizeä¸º(bsz, self.num_heads, q_len, self.head_dim)ï¼Œåœ¨ç¬¬1ç»´åº¦ä¸Šä¸å¯¹é½ï¼Œä¸¤ä¸ªå‚æ•°ä¹‹é—´çš„å…³ç³»ä¸ºself.num_kv_groups = self.num_heads // self.num_kv_headsã€‚å› æ­¤æˆ‘ä»¬åœ¨å¯¹é½çš„ä»£ç å®ç°ä¸Šï¼Œé¦–å…ˆæ’å…¥ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œå¤§å°ä¸ºself.num_kv_groupsï¼Œé€šè¿‡flattenæ“ä½œå°†ç¬¬1ã€2ç»´åº¦å±•å¹³ï¼Œæ–°çš„ç¬¬1ç»´åº¦å¤§å°å°±å’ŒQçš„ç»´åº¦å®Œæˆäº†å¯¹é½ã€‚
- åœ¨å®Œæˆè¿ç®—åï¼Œæˆ‘ä»¬é¦–å…ˆå°†ä¹‹å‰é¢ å€’çš„1ã€2ç»´åº¦å¤åŸï¼Œç„¶åå†å°†ä¹‹å‰æ‹†å¼€çš„hidden_sizeæ‹¼å¥½ï¼Œæœ€åé€šè¿‡o_projè¿”å›GQAçš„ç»“æœ
#### Rope
```python
def apply_rope(q, k, position_ids, head_dim, rope_theta=1000000.0):
	device = q.device
	inv_freq = 1.0 / (rope_theta ** (torch.arange(0, head_dim, 2, dtype=torch.float32, device=device) / head_dim))
	freqs = position_ids.unsqueeze(-1).float() * inv_freq.unsqueeze(0).unsqueeze(0)
	emb = torch.cat([freqs, freqs], dim=-1)
	cos = emb.cos().unsqueeze(1).to(q.dtype)
	sin = emb.sin().unsqueeze(1).to(q.dtype)
	
	def rotate_half(x):
		x1, x2 = x.chunk(2, dim=-1)
		return torch.cat((-x2, x1), dim=-1)
		
	q_embed = (q * cos) + (rotate_half(q) * sin)
	k_embed = (k * cos) + (rotate_half(k) * sin)
	return q_embed, k_embed
```
- Ropeæ­¤å¤„ä¸ä½œè®²è§£ï¼Œåç»­å†™æ–‡ç« è®¨è®ºï¼Œç¯‡å¹…è¾ƒé•¿
#### MLP
```python
class MLP(nn.Module):
	def __init__(self, config):
		super().__init__()
		self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
		self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
		self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)

	def forward(self, x):
		ret = self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))
		return ret
```
- MLPéƒ¨åˆ†ä¸ºä¸‰ä¸ªçº¿æ€§å±‚ï¼ŒæŒ‰æµç¨‹è®¡ç®—å³å¯
### æ¨ç†
åœ¨å®ç°å®Œä¸Šè¿°çš„æ¶æ„ä¹‹åï¼Œæˆ‘ä»¬åªéœ€è¦å°†æƒé‡loadè¿›å»ï¼Œå³å¯è¿›è¡Œæ¨ç†è¿‡ç¨‹ï¼š
```python
from tokenizers import Tokenizer
config = AutoConfig.from_pretrained("./Qwen3-0.6B/")
model = Qwen3Model(config)
new_state_dict = {}
for k, v in state_dict.items():
	if k.startswith("model."):
		new_state_dict[k[len("model") + 1:]] = v
	else:
		new_state_dict[k] = v

model.load_state_dict(new_state_dict, strict=True)
model.eval()

tokenizer = Tokenizer.from_file(str("./Qwen3-0.6B/tokenizer.json"))
message="<|im_start|>userä½ å¥½ï¼Œæˆ‘æ˜¯ComistryMoï¼Œè¯·å¤šæŒ‡æ•™ï¼<|im_end|><|im_start|>assistant"
input_ids = tokenizer.encode(message).ids
input_ids = torch.tensor([input_ids], dtype=torch.long)
with torch.no_grad():
	while True:
		logits = model(input_ids)
		next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
		if next_token.item() == 151645:
			break
		input_ids = torch.cat([input_ids, next_token], dim=1)

output_text = tokenizer.decode(input_ids[0].tolist(), skip_special_tokens=True)
print(output_text)
```
- æµç¨‹å¦‚ä¸‹ï¼š
![[Pasted image 20251120213910.png]]
æœ€ç»ˆç»“æœå¦‚ä¸‹ï¼š
```python
begin init model 
userä½ å¥½ï¼Œæˆ‘æ˜¯ComistryMoï¼Œè¯·å¤šæŒ‡æ•™ï¼assistant<think> 
<think> 
å¥½çš„ï¼Œç”¨æˆ·æ˜¯ComistryMoï¼Œçœ‹èµ·æ¥åƒæ˜¯ä¸€ä¸ªåŒ–å¦†å“å“ç‰Œæˆ–è€…ç›¸å…³é¢†åŸŸçš„ç”¨æˆ·ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ã€‚ç”¨æˆ·å¯èƒ½æ˜¯åœ¨è¯¢é—®å…³äºåŒ–å¦†å“çš„ä½¿ç”¨æ–¹æ³•ã€äº§å“æ¨èï¼Œæˆ–è€…é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚ç”±äºç”¨æˆ·æ²¡æœ‰æä¾›è¯¦ç»†çš„é—®é¢˜ï¼Œæˆ‘éœ€è¦ä¿æŒå‹å¥½å’Œå¼€æ”¾çš„æ€åº¦ï¼Œå¼•å¯¼ä»–ä»¬æ›´å…·ä½“åœ°è¯´æ˜é—®é¢˜ã€‚ 
æ¥ä¸‹æ¥ï¼Œæˆ‘åº”è¯¥è€ƒè™‘å¦‚ä½•å›åº”ã€‚ç”¨æˆ·å¯èƒ½å¸Œæœ›å¾—åˆ°å¸®åŠ©ï¼Œä½†éœ€è¦å…ˆäº†è§£ä»–ä»¬çš„å…·ä½“éœ€æ±‚ã€‚å› æ­¤ï¼Œæˆ‘å¯ä»¥ç”¨å‹å¥½çš„æ–¹å¼è¯¢é—®ï¼Œæ¯”å¦‚â€œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿâ€æˆ–è€…â€œéœ€è¦æˆ‘å¸®ä½ è§£ç­”ä»€ä¹ˆé—®é¢˜å—ï¼Ÿâ€è¿™æ ·å¯ä»¥ç¡®ä¿ç”¨æˆ·èƒ½å¤Ÿæä¾›æ›´å¤šä¿¡æ¯ï¼Œä»è€Œæä¾›æ›´å‡†ç¡®çš„å¸®åŠ©ã€‚ 
åŒæ—¶ï¼Œæˆ‘éœ€è¦ç¡®ä¿å›åº”çš„è¯­æ°”ä¸“ä¸šä¸”äº²åˆ‡ï¼Œè®©ç”¨æˆ·æ„Ÿåˆ°è¢«é‡è§†ã€‚é¿å…ä½¿ç”¨è¿‡äºæŠ€æœ¯åŒ–çš„æœ¯è¯­ï¼Œè®©ç”¨æˆ·å®¹æ˜“ç†è§£å’Œæ¥å—ã€‚æ­¤å¤–ï¼Œä¿æŒå›ç­”ç®€æ´æ˜äº†ï¼Œé¿å…å†—é•¿ï¼Œè®©ç”¨æˆ·èƒ½å¤Ÿå¿«é€Ÿè·å–æ‰€éœ€çš„ä¿¡æ¯ã€‚ 
æœ€åï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ä¿¡æ¯ï¼Œç¡®ä¿å›ç­”å…¨é¢ä¸”ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ã€‚å¦‚æœç”¨æˆ·æœ‰å…¶ä»–é—®é¢˜ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥å¼•å¯¼ä»–ä»¬ï¼Œä»¥ç¡®ä¿é—®é¢˜å¾—åˆ°å……åˆ†çš„è§£ç­”ã€‚ 
</think> 

ä½ å¥½ï¼æˆ‘æ˜¯ComistryMoï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼ŸğŸ˜Š å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¯”å¦‚äº§å“ä½¿ç”¨ã€äº§å“æ¨èï¼Œæˆ–è€…é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼
```
### ä¸å®˜æ–¹çš„æ€§èƒ½å¯¹æ¯”
ä»£ç å¦‚ä¸‹ï¼š
```python
import time
import torch
from tokenizers import Tokenizer
from transformers import AutoConfig, AutoModelForCausalLM
from safetensors.torch import load_file

device = "cuda" if torch.cuda.is_available() else "cpu"
path = "./Qwen3-0.6B/"

config = AutoConfig.from_pretrained(path)
my_model = Qwen3Model(config).to(device) 

state_dict = load_file(f"{path}/model.safetensors", device=device)
new_state_dict = {}
for k, v in state_dict.items():
    if k.startswith("model."):
        new_state_dict[k[len("model") + 1:]] = v
    else:
        new_state_dict[k] = v
my_model.load_state_dict(new_state_dict, strict=True)
my_model.eval()

official_model = AutoModelForCausalLM.from_pretrained(
    path, 
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    trust_remote_code=True
).to(device)
official_model.eval()

tokenizer = Tokenizer.from_file(str(f"{path}/tokenizer.json"))
message = "<|im_start|>userä½ å¥½ï¼Œæˆ‘æ˜¯ComistryMoï¼Œè¯·å¤šæŒ‡æ•™ï¼<|im_end|><|im_start|>assistant"
input_ids_raw = tokenizer.encode(message).ids
input_ids = torch.tensor([input_ids_raw], dtype=torch.long).to(device)

def measure_time(func, name):
    # é¢„çƒ­ä¸€æ¬¡ï¼Œé˜²æ­¢ç¬¬ä¸€æ¬¡è¿è¡ŒåŒ…å«äº†åˆå§‹åŒ–å¼€é”€
    print(f"æ­£åœ¨é¢„çƒ­ {name}...")
    if device == "cuda":
        torch.cuda.synchronize()
    
    start = time.perf_counter()
    result = func()
    
    if device == "cuda":
        torch.cuda.synchronize()
    end = time.perf_counter()
    
    elapsed = end - start
    print(f"[{name}] è€—æ—¶: {elapsed:.4f} ç§’")
    return result

def run_my_inference():
    curr_input = input_ids.clone()
    with torch.no_grad():
        while True:
            logits = my_model(curr_input)
            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
            if next_token.item() == 151645:
                break

            curr_input = torch.cat([curr_input, next_token], dim=1)
            
            if curr_input.shape[1] > 200: 
                break
    return tokenizer.decode(curr_input[0].tolist(), skip_special_tokens=True)

def run_official_inference():
    with torch.no_grad():
        output = official_model.generate(
            input_ids, 
            max_new_tokens=200,
            eos_token_id=151645,
            pad_token_id=151645,
            use_cache=True
        )
    return tokenizer.decode(output[0].tolist(), skip_special_tokens=True)

print("--- å¼€å§‹å¯¹æ¯” ---")
output_my = measure_time(run_my_inference, "æ‰‹åŠ¨æ¨ç† (æ— KV Cache)")
output_official = measure_time(run_official_inference, "å®˜æ–¹æ¨ç† (æœ‰KV Cache)")

print("\n--- ç»“æœéªŒè¯ ---")
print(f"æ‰‹åŠ¨ç»“æœé•¿åº¦: {len(output_my)}")
print(f"å®˜æ–¹ç»“æœé•¿åº¦: {len(output_official)}")
```
- åœ¨å¼€å¯å®˜æ–¹çš„kv_cacheä¸‹ï¼Œç»“æœå¦‚ä¸‹ï¼š
![[Pasted image 20251120215620.png]]
- è‹¥å…³é—­kv_cacheï¼Œç»“æœåˆ™å˜ä¸ºæ‰‹åŠ¨æ¨ç†è€—æ—¶79.4075ç§’ï¼Œå®˜æ–¹è€—æ—¶54.9702ç§’
- å¯ä»¥çœ‹åˆ°kv_cacheå¸¦æ¥çš„å½±å“è¿˜æ˜¯å¾ˆå¤§çš„ï¼Œåç»­ä¼šå®ç°å¸¦kv_cacheç‰ˆçš„æ¨ç†demoä»¥åŠå¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¨ç†demo